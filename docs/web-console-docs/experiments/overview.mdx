---
sidebar_position: 1
---

import Image from "../../../src/components/Image";

# Overview

## What is an experiment

An experiment evaluates the impact of a product change by comparing the behaviour of users who see the change with those who do not. 
ABsmartly makes this possible by assigning users to variants, tracking their actions with goal events, 
and analysing the difference between variants with reliable statistics.

An experiment contains several key elements:

- **Exposure**
How users enter the experiment and how they are assigned to variants.

- **Variants**
Different experiences shown to users.

- **Goals and Metrics**
Events and measurements used to evaluate the impact.

- **Monitoring**
Automatic checks that help ensure the experiment is healthy and safe to run.

- **Results and Decisions**
The analysis of variant impact and the decision you take once the data is clear.

ABsmartly handles all randomisation, data collection, metric computation, and statistical inference so you can focus on learning from your product changes.

### Exposure and assignment

When a user reaches an experiment, ABsmartly assigns them to a variant using a deterministic hashing method. This ensures:
- stable assignment
- no cross contamination across variants
- consistent behaviour during the entire experiment
- predictable control of traffic allocation

[Exposure events](../events/exposure-events) are sent automatically by the SDK, and these events define when users become part of the analysis dataset.

### Variants

Experiments usually include a control variant and one or more treatment variants. Each variant represents a specific user experience. 
ABsmartly allows you to configure:
- variant names
- traffic allocation
- rollout rules
- targeting rules
- experiment overrides for testing or QA

Variants determine what users see, while the metrics determine how those differences are evaluated.

## Goals and metrics

Experiments are measured using [goals](../goals-and-metrics/goals/overview) and the [metrics](../goals-and-metrics/metrics/overview) derived from them.

Examples of goals:

```javascript
context.track("purchase", { price: 1000 });
context.track("add_to_cart", { product_id: "ABC123" });
context.track("view_item", { item_id: "XYZ987" });
```

Metrics take your [goal events](../Events/goal-events) and turn them into meaningful measurements of user behaviour. 
They let you answer questions like:

- how many times something happened
- how many users performed an action
- how much value was generated
- how behaviour changes between variants

ABsmartly handles all the computation and presents the results in a clear, comparable way across variants.

### Guardrails and monitoring

Before experiment results can be trusted, ABsmartly performs several [health checks](Experiment-health-checks) automatically:
- Sample Ratio Mismatch at the experiment level
- assignment and exposure conflicts
- guardrail metric thresholds
- unexpected behaviour in exposure or goal events
- data quality anomalies

Guardrails help detect harmful side effects and make it possible to [abort experiments](Aborting-experiments) if needed.

You can also configure guardrail metrics with [thresholds](../goals-and-metrics/metrics/create#metric-threshold-alert) that alert you when impact crosses meaningful limits, 
for example when a performance metric becomes slower or when a business KPI shows a potential drop.

### Analysing results

ABsmartly uses sound statistical methods to compute the impact of each variant compared to control. For each metric you will see:
- visitor count 
- total value
- mean value per user
- relative impact
- confidence interval
- impact per day

You can use our guide on [interpreting metrics results](interpreting-metrics-in-experiment-results) to help with the analysis step.

### Decisions

Once results are stable and clear, you can record a decision:

**Full on**
The treatment performs well enough to roll out fully.

**Keep Current**
Keep the current implementation and either **iterate** or **abandon**.

**Abort**
The experiment produced a harmful effect or a technical issue made results invalid.

Decisions are tracked across the platform to support transparency, accountability, and long term learning.

## Analysis methods

ABsmartly supports two types of statistical analysis: fixed horizon and group sequential. 
Both methods compare variant performance, but they differ in how and when you can look at the results.
Understanding the differences between these methods and knowing when to use each can significantly impact the efficiency and accuracy of your experimentation program.

### Fixed horizon

Fixed Horizon Testing involves analyzing the results of an experiment after reaching a predefined sample size (number of unique visitors) or 
reaching a specific duration. This method, supported by most AB Testing tools, assumes that the sample size is defined before the experiment starts and 
remains unchanged throughout the runtime of the experiment.

While this method is widely used and beneficial, it lacks flexibility, as decisions can only be made at a single predefined moment. 
This limitation can lead to unreliable decisions (when experimenters make decisions too early) as well as wasted time and resources. 
This makes the use of Fixed Horizon testing for product experimentation, where trust, speed, and agility are crucial, less beneficial and more challenging. 
This is especially true for teams with less experience.

Fixed horizon uses a 2-sided test, meaning it evaluates whether the observed effect is significantly in either direction (positive or negative). 
Results in a 2 sided-test can be significantly positive, significantly negative or insignificant.

### Group sequential

[Group Sequential Testing](https://absmartly.com/gst) is an adaptive analysis method that allows for interim analyses at various points during the experiment. 
At ABsmartly you can decide how often or how many interim analyses you want. 
A Group Sequential approach provides the flexibility to stop the experiments early for efficacy or for futility.

<Image img="gst-setup.png" alt="Setting up a Group Sequential Test" maxWidth="40rem" />

While adding more interim analysis will slightly reduce statistical power compared to fixed-horizon testing, overall it greatly speeds up decision-making, as significance is commonly reached before the full sample is collected. This efficiency gained from using Group Sequential Testing is making a real difference to ABsmartly customers, to the pace at which decisions can be made.

<Image img="gst-widget.png" alt="A Group Sequential Test result" maxWidth="40rem" />

Unlike Fixed Horizon, Group Sequential Testing uses a 1-sided test, meaning it evaluates whether the observed effect is significant only in the expected direction. Results in a 1-sided test can either be significant in the expected direction or insignificant.

:::info
Different experimentation platforms might use different sequential testing implementation. 
The most commonly used sequential method is Fully Sequential and while it offers the most flexibility (decisions can be made at any moment in time), 
it comes at the cost of much lower power which in turn leads to higher time to decision. 
At ABsmartly we believe Group Sequential Testing provides the right compromise between flexibility and speed which is required to make high-quality 
product decisions in a fast-moving business context.
:::

## Which one to choose?

Both methods ensure reliable results, but group sequential analysis provides more flexibility, while fixed horizon follows a more traditional “run to completion” approach.
Most of the time, Group Sequential Testing should be the preferred method (who does not want faster trustworthy results?) but there are a few use cases where you might decide to use a Fixed Horizon setup. This is mainly when you are dealing with a strong novelty effect (Group Sequential Testing might come to a premature conclusion which might not reflect the true impact) or where you have a long action cycle and wish to observe the visitors for a pre-defined period of time.

Because it is a 2-sided test, Fixed Horizon is a better choice if differentiating between inconclusive and significantly negative results is important. 


## Experiment lifecycle


## Ownership & permissions